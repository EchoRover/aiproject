{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b64d20d",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Energy Datasets\n",
    "\n",
    "**Project:** AI Final Year Project  \n",
    "**Date:** December 2025  \n",
    "**Objective:** Initial exploration of energy datasets to understand their characteristics and suitability for showcasing ML algorithms\n",
    "\n",
    "## Datasets to Analyze:\n",
    "1. `ENB2012_data.xlsx` - Energy Efficiency Dataset\n",
    "2. `energydata_complete.csv` - Appliance Energy Prediction Dataset\n",
    "\n",
    "## Goals:\n",
    "- Understand dataset sizes and structure\n",
    "- Check data quality (missing values, types)\n",
    "- Identify suitable algorithms for each dataset\n",
    "- Determine which datasets should be in Git vs downloaded separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb313efd",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adec9bb",
   "metadata": {},
   "source": [
    "## 2. Check Dataset Sizes\n",
    "\n",
    "First, let's check the file sizes to understand which datasets are large and should be handled separately from Git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "dataset_dir = Path('../datasets')\n",
    "datasets = [\n",
    "    'ENB2012_data.xlsx',\n",
    "    'energydata_complete.csv'\n",
    "]\n",
    "\n",
    "# Check file sizes\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET SIZE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_info = []\n",
    "for dataset in datasets:\n",
    "    filepath = dataset_dir / dataset\n",
    "    if filepath.exists():\n",
    "        size_bytes = filepath.stat().st_size\n",
    "        size_mb = size_bytes / (1024 * 1024)\n",
    "        git_track = \"YES (< 10MB)\" if size_mb < 10 else \"NO (> 10MB) - Download separately\"\n",
    "        \n",
    "        dataset_info.append({\n",
    "            'Dataset': dataset,\n",
    "            'Size (MB)': round(size_mb, 2),\n",
    "            'Track in Git': git_track\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüìä {dataset}\")\n",
    "        print(f\"   Size: {size_mb:.2f} MB\")\n",
    "        print(f\"   Git Tracking: {git_track}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {dataset} - FILE NOT FOUND\")\n",
    "        dataset_info.append({\n",
    "            'Dataset': dataset,\n",
    "            'Size (MB)': 'N/A',\n",
    "            'Track in Git': 'File not found'\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_sizes = pd.DataFrame(dataset_info)\n",
    "print(\"\\nüìã SUMMARY TABLE:\")\n",
    "print(df_sizes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d378e",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect Dataset 1: ENB2012_data.xlsx\n",
    "\n",
    "Energy Efficiency Dataset - Building characteristics and energy consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 1\n",
    "try:\n",
    "    df1 = pd.read_excel(dataset_dir / 'ENB2012_data.xlsx')\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATASET 1: ENB2012_data.xlsx - Energy Efficiency\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüìê Shape: {df1.shape[0]} rows √ó {df1.shape[1]} columns\")\n",
    "    print(f\"\\nüìã Columns:\")\n",
    "    for i, col in enumerate(df1.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüîç First 5 rows:\")\n",
    "    display(df1.head())\n",
    "    \n",
    "    print(f\"\\nüìä Data Types:\")\n",
    "    display(df1.dtypes)\n",
    "    \n",
    "    print(f\"\\nüíæ Memory Usage:\")\n",
    "    print(df1.memory_usage(deep=True).sum() / 1024, \"KB\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ENB2012_data.xlsx not found in datasets folder\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f51477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1: Basic Statistics\n",
    "if 'df1' in locals():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATASET 1: BASIC STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    display(df1.describe())\n",
    "    \n",
    "    print(\"\\n‚úÖ Missing Values:\")\n",
    "    missing = df1.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"   No missing values found!\")\n",
    "    else:\n",
    "        display(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada58a0a",
   "metadata": {},
   "source": [
    "## 4. Load and Inspect Dataset 2: energydata_complete.csv\n",
    "\n",
    "Appliance Energy Prediction Dataset - Time-series energy consumption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 2\n",
    "try:\n",
    "    df2 = pd.read_csv(dataset_dir / 'energydata_complete.csv')\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATASET 2: energydata_complete.csv - Appliance Energy Prediction\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüìê Shape: {df2.shape[0]} rows √ó {df2.shape[1]} columns\")\n",
    "    print(f\"\\nüìã Columns:\")\n",
    "    for i, col in enumerate(df2.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüîç First 5 rows:\")\n",
    "    display(df2.head())\n",
    "    \n",
    "    print(f\"\\nüîç Last 5 rows:\")\n",
    "    display(df2.tail())\n",
    "    \n",
    "    print(f\"\\nüìä Data Types:\")\n",
    "    display(df2.dtypes)\n",
    "    \n",
    "    print(f\"\\nüíæ Memory Usage:\")\n",
    "    print(f\"{df2.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå energydata_complete.csv not found in datasets folder\")\n",
    "    print(\"   This file should be downloaded separately (see DATA_SOURCES.md)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2: Basic Statistics\n",
    "if 'df2' in locals():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATASET 2: BASIC STATISTICS\")\n",
    "    print(\"=\" * 70)\n",
    "    display(df2.describe())\n",
    "    \n",
    "    print(\"\\n‚úÖ Missing Values:\")\n",
    "    missing = df2.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"   No missing values found!\")\n",
    "    else:\n",
    "        print(f\"   Total missing values: {missing.sum()}\")\n",
    "        display(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e035d85",
   "metadata": {},
   "source": [
    "## 5. Compare Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare datasets\n",
    "comparison_data = []\n",
    "\n",
    "if 'df1' in locals():\n",
    "    comparison_data.append({\n",
    "        'Dataset': 'ENB2012_data.xlsx',\n",
    "        'Rows': df1.shape[0],\n",
    "        'Columns': df1.shape[1],\n",
    "        'Numeric Cols': len(df1.select_dtypes(include=[np.number]).columns),\n",
    "        'Categorical Cols': len(df1.select_dtypes(exclude=[np.number]).columns),\n",
    "        'Missing Values': df1.isnull().sum().sum(),\n",
    "        'Memory (MB)': round(df1.memory_usage(deep=True).sum() / (1024*1024), 2)\n",
    "    })\n",
    "\n",
    "if 'df2' in locals():\n",
    "    comparison_data.append({\n",
    "        'Dataset': 'energydata_complete.csv',\n",
    "        'Rows': df2.shape[0],\n",
    "        'Columns': df2.shape[1],\n",
    "        'Numeric Cols': len(df2.select_dtypes(include=[np.number]).columns),\n",
    "        'Categorical Cols': len(df2.select_dtypes(exclude=[np.number]).columns),\n",
    "        'Missing Values': df2.isnull().sum().sum(),\n",
    "        'Memory (MB)': round(df2.memory_usage(deep=True).sum() / (1024*1024), 2)\n",
    "    })\n",
    "\n",
    "if comparison_data:\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"=\" * 90)\n",
    "    print(\"DATASET COMPARISON\")\n",
    "    print(\"=\" * 90)\n",
    "    display(df_comparison)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Number of rows\n",
    "    axes[0].bar(df_comparison['Dataset'], df_comparison['Rows'], color=['#3498db', '#e74c3c'])\n",
    "    axes[0].set_title('Dataset Size (Number of Rows)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Number of Rows')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Number of columns\n",
    "    axes[1].bar(df_comparison['Dataset'], df_comparison['Columns'], color=['#2ecc71', '#f39c12'])\n",
    "    axes[1].set_title('Dataset Dimensions (Number of Columns)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Number of Columns')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045f72a",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis - Dataset 1\n",
    "\n",
    "Understanding relationships between features helps us choose appropriate ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for Dataset 1\n",
    "if 'df1' in locals():\n",
    "    print(\"CORRELATION ANALYSIS - Dataset 1\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df1.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Calculate correlation\n",
    "        corr_matrix = df1[numeric_cols].corr()\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                    center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "        plt.title('Correlation Matrix - ENB2012 Energy Efficiency Dataset', \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find highly correlated features (> 0.7 or < -0.7)\n",
    "        print(\"\\nüî• Highly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "        high_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                    high_corr.append({\n",
    "                        'Feature 1': corr_matrix.columns[i],\n",
    "                        'Feature 2': corr_matrix.columns[j],\n",
    "                        'Correlation': round(corr_matrix.iloc[i, j], 3)\n",
    "                    })\n",
    "        \n",
    "        if high_corr:\n",
    "            display(pd.DataFrame(high_corr))\n",
    "        else:\n",
    "            print(\"   No highly correlated pairs found.\")\n",
    "    else:\n",
    "        print(\"No numeric columns found for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5bb11",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis - Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for Dataset 2\n",
    "if 'df2' in locals():\n",
    "    print(\"CORRELATION ANALYSIS - Dataset 2\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df2.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # For large datasets, show top correlations with target variable if it exists\n",
    "        # Assuming 'Appliances' or similar is the target\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr_matrix = df2[numeric_cols].corr()\n",
    "        \n",
    "        # Plot correlation heatmap (might be large, so use smaller figure for many features)\n",
    "        if len(numeric_cols) > 15:\n",
    "            fig_size = (16, 14)\n",
    "            annot_size = 6\n",
    "        else:\n",
    "            fig_size = (12, 10)\n",
    "            annot_size = 8\n",
    "            \n",
    "        plt.figure(figsize=fig_size)\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                    center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "                    annot_kws={'size': annot_size})\n",
    "        plt.title('Correlation Matrix - Appliance Energy Prediction Dataset', \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show top correlations if there's a clear target variable\n",
    "        print(\"\\nüìä Correlation with potential target variables:\")\n",
    "        for col in numeric_cols[:5]:  # Check first 5 columns\n",
    "            if 'energy' in col.lower() or 'appliance' in col.lower() or 'load' in col.lower():\n",
    "                print(f\"\\nTop correlations with '{col}':\")\n",
    "                corr_with_target = corr_matrix[col].abs().sort_values(ascending=False)[1:11]\n",
    "                display(corr_with_target)\n",
    "    else:\n",
    "        print(\"No numeric columns found for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb063f8",
   "metadata": {},
   "source": [
    "## 8. Initial Insights and ML Algorithm Suitability\n",
    "\n",
    "Based on the exploratory analysis, let's determine which algorithms are suitable for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"INITIAL INSIGHTS & ML ALGORITHM RECOMMENDATIONS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "insights = []\n",
    "\n",
    "if 'df1' in locals():\n",
    "    print(\"\\nüìä DATASET 1: ENB2012_data.xlsx\")\n",
    "    print(\"-\" * 90)\n",
    "    print(\"‚úÖ Suitable for:\")\n",
    "    print(\"   ‚Ä¢ LINEAR REGRESSION - Predict heating/cooling loads from building features\")\n",
    "    print(\"   ‚Ä¢ POLYNOMIAL REGRESSION - Capture non-linear relationships\")\n",
    "    print(\"   ‚Ä¢ DECISION TREES - Handle feature interactions\")\n",
    "    print(\"   ‚Ä¢ RANDOM FOREST - Improve prediction accuracy\")\n",
    "    print(\"   ‚Ä¢ NEURAL NETWORKS (PyTorch) - Deep learning approach\")\n",
    "    print(\"   ‚Ä¢ CLUSTERING (K-means) - Group similar buildings\")\n",
    "    print(\"\\nüí° Insights:\")\n",
    "    print(f\"   ‚Ä¢ {df1.shape[0]} building samples\")\n",
    "    print(f\"   ‚Ä¢ {df1.shape[1]} features\")\n",
    "    print(f\"   ‚Ä¢ Clean data - no missing values\")\n",
    "    print(\"   ‚Ä¢ All numeric features - ready for ML\")\n",
    "    print(\"   ‚Ä¢ Multiple targets possible (heating and cooling loads)\")\n",
    "\n",
    "if 'df2' in locals():\n",
    "    print(\"\\n\\nüìä DATASET 2: energydata_complete.csv\")\n",
    "    print(\"-\" * 90)\n",
    "    print(\"‚úÖ Suitable for:\")\n",
    "    print(\"   ‚Ä¢ LINEAR REGRESSION - Predict appliance energy consumption\")\n",
    "    print(\"   ‚Ä¢ LOGISTIC REGRESSION - Binary classification (high/low energy)\")\n",
    "    print(\"   ‚Ä¢ DECISION TREES - Capture temporal patterns\")\n",
    "    print(\"   ‚Ä¢ NEURAL NETWORKS (PyTorch) - Time-series modeling\")\n",
    "    print(\"   ‚Ä¢ CLUSTERING (K-means) - Identify energy consumption patterns\")\n",
    "    print(\"\\nüí° Insights:\")\n",
    "    print(f\"   ‚Ä¢ {df2.shape[0]} time-series observations\")\n",
    "    print(f\"   ‚Ä¢ {df2.shape[1]} features (temperature, humidity, weather data)\")\n",
    "    if df2.isnull().sum().sum() == 0:\n",
    "        print(\"   ‚Ä¢ Clean data - no missing values\")\n",
    "    print(\"   ‚Ä¢ Rich feature set for complex modeling\")\n",
    "    print(\"   ‚Ä¢ Can create classification problems from regression\")\n",
    "\n",
    "print(\"\\n\\nüéØ PROJECT STRATEGY:\")\n",
    "print(\"-\" * 90)\n",
    "print(\"To maximize marks, we can:\")\n",
    "print(\"1. Use BOTH datasets to showcase different problem types\")\n",
    "print(\"2. Dataset 1: Focus on regression algorithms and model comparison\")\n",
    "print(\"3. Dataset 2: Create both regression AND classification problems\")\n",
    "print(\"4. Apply clustering to both datasets for unsupervised learning\")\n",
    "print(\"5. Use PyTorch neural networks on at least one dataset\")\n",
    "print(\"6. Compare all algorithms using appropriate metrics (MSE, R¬≤, Accuracy, etc.)\")\n",
    "print(\"\\n‚úÖ This covers ALL algorithms learned in your course!\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118a3a6",
   "metadata": {},
   "source": [
    "## 9. Save Summary for Project Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary for documentation\n",
    "summary = {\n",
    "    'analysis_date': '2025-12-07',\n",
    "    'datasets_analyzed': []\n",
    "}\n",
    "\n",
    "if 'df1' in locals():\n",
    "    summary['datasets_analyzed'].append({\n",
    "        'name': 'ENB2012_data.xlsx',\n",
    "        'rows': df1.shape[0],\n",
    "        'columns': df1.shape[1],\n",
    "        'missing_values': df1.isnull().sum().sum(),\n",
    "        'suitable_algorithms': ['Linear Regression', 'Decision Trees', 'Neural Networks', 'K-means']\n",
    "    })\n",
    "\n",
    "if 'df2' in locals():\n",
    "    summary['datasets_analyzed'].append({\n",
    "        'name': 'energydata_complete.csv',\n",
    "        'rows': df2.shape[0],\n",
    "        'columns': df2.shape[1],\n",
    "        'missing_values': df2.isnull().sum().sum(),\n",
    "        'suitable_algorithms': ['Linear Regression', 'Logistic Regression', 'Decision Trees', 'Neural Networks', 'K-means']\n",
    "    })\n",
    "\n",
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(f\"   ‚Ä¢ {len(summary['datasets_analyzed'])} datasets analyzed\")\n",
    "print(f\"   ‚Ä¢ Ready to proceed with ML model development\")\n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"   1. Create detailed preprocessing notebooks\")\n",
    "print(\"   2. Implement regression models\")\n",
    "print(\"   3. Implement classification models\")\n",
    "print(\"   4. Implement clustering analysis\")\n",
    "print(\"   5. Implement neural networks with PyTorch\")\n",
    "print(\"   6. Compare and evaluate all models\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
