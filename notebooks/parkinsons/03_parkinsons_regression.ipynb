{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800ebb1e",
   "metadata": {},
   "source": [
    "# Parkinson's Disease - Regression Modeling (Predicting UPDRS Scores)\n",
    "## Stage 3: Training and Evaluating Regression Models\n",
    "\n",
    "### üéØ What are we predicting in this notebook?\n",
    "We are building **regression models** to predict disease severity scores from voice recordings:\n",
    "\n",
    "**REGRESSION TASK 1: Predict motor_UPDRS**\n",
    "- **What is motor_UPDRS?** Movement symptom score (scale 0-108)\n",
    "  - Measures: Tremor, rigidity, slowness, walking ability\n",
    "  - Higher score = worse movement symptoms\n",
    "- **Our goal:** Predict this score using only voice features\n",
    "- **Why?** Allow patients to monitor disease from home without clinic visits\n",
    "\n",
    "**REGRESSION TASK 2: Predict total_UPDRS**\n",
    "- **What is total_UPDRS?** Overall disease severity score (scale 0-176)\n",
    "  - Includes: Motor symptoms + mental state + daily activities\n",
    "  - Comprehensive measure of disease impact\n",
    "- **Our goal:** Predict this score from voice recordings\n",
    "- **Expected:** Harder to predict than motor_UPDRS (includes non-motor symptoms)\n",
    "\n",
    "### üîç How will we evaluate models?\n",
    "We'll use multiple metrics to compare model performance:\n",
    "\n",
    "1. **R¬≤ Score (R-squared)** - Primary metric\n",
    "   - Measures: % of variance in UPDRS explained by voice features\n",
    "   - Range: 0 to 1 (higher is better)\n",
    "   - Example: R¬≤ = 0.87 means model explains 87% of UPDRS variation\n",
    "   - **Clinical target:** R¬≤ > 0.80 (80% accuracy needed for home monitoring)\n",
    "\n",
    "2. **RMSE (Root Mean Squared Error)**\n",
    "   - Measures: Average prediction error in UPDRS points\n",
    "   - Units: Same as UPDRS (points)\n",
    "   - Example: RMSE = 3.5 means predictions are off by ~3.5 points on average\n",
    "   - **Clinical target:** RMSE < 4 points (acceptable for monitoring)\n",
    "\n",
    "3. **MAE (Mean Absolute Error)**\n",
    "   - Measures: Typical prediction error (less sensitive to outliers than RMSE)\n",
    "   - Units: UPDRS points\n",
    "   - Example: MAE = 2.8 means typical error is 2.8 points\n",
    "   - **Why useful:** Easier to interpret than RMSE\n",
    "\n",
    "4. **MAPE (Mean Absolute Percentage Error)**\n",
    "   - Measures: Error as percentage of actual value\n",
    "   - Example: MAPE = 12% means predictions are typically 12% off\n",
    "   - **Why useful:** Scale-independent comparison\n",
    "\n",
    "5. **Residual Analysis**\n",
    "   - **What:** Difference between predicted and actual values (prediction error)\n",
    "   - **Why:** Check if errors are random or have patterns\n",
    "   - **Good model:** Residuals centered at 0, no patterns\n",
    "   - **Bad model:** Residuals show trends (model is biased)\n",
    "\n",
    "### ü§ñ Models we'll compare (5 total):\n",
    "1. **Linear Regression** - Baseline (assumes linear relationships)\n",
    "2. **Polynomial Regression** - Adds interaction terms\n",
    "3. **Decision Tree** - Handles non-linearity\n",
    "4. **Random Forest** - Ensemble (expected winner!)\n",
    "5. **Neural Network** - Deep learning approach\n",
    "\n",
    "### üìä What we'll discover:\n",
    "- Which voice features are most important for prediction?\n",
    "- Why Random Forest beats Linear Regression (non-linear relationships!)\n",
    "- Is motor_UPDRS easier to predict than total_UPDRS?\n",
    "- How much error can we expect (RMSE in UPDRS points)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1: IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "# What we're doing: Loading all ML and visualization libraries\n",
    "# Why: Need scikit-learn for models, PyTorch for neural networks, pandas for data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Scikit-learn metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch for Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73905ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: LOAD PREPROCESSED DATA\n",
    "# ============================================================\n",
    "# What we're doing: Loading the train/test data we created in preprocessing notebook\n",
    "# Why: Data is already scaled and split (no need to repeat preprocessing)\n",
    "#\n",
    "# Files we're loading:\n",
    "# - X_train.csv: Training features (4,700 recordings √ó 26 features)\n",
    "# - X_test.csv: Test features (1,175 recordings √ó 26 features)\n",
    "# - y_train_motor.csv: Training target for motor_UPDRS\n",
    "# - y_test_motor.csv: Test target for motor_UPDRS\n",
    "# - y_train_total.csv: Training target for total_UPDRS\n",
    "# - y_test_total.csv: Test target for total_UPDRS\n",
    "\n",
    "print(\"Loading preprocessed data...\\\\n\")\n",
    "\n",
    "# Load features (X)\n",
    "X_train = pd.read_csv('../../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test.csv')\n",
    "\n",
    "# Load motor_UPDRS targets\n",
    "y_train_motor = pd.read_csv('../../data/processed/y_train_motor.csv').values.ravel()\n",
    "y_test_motor = pd.read_csv('../../data/processed/y_test_motor.csv').values.ravel()\n",
    "\n",
    "# Load total_UPDRS targets\n",
    "y_train_total = pd.read_csv('../../data/processed/y_train_total.csv').values.ravel()\n",
    "y_test_total = pd.read_csv('../../data/processed/y_test_total.csv').values.ravel()\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\\\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DATASET SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\\\nTraining set:\")\n",
    "print(f\"  X_train shape:      {X_train.shape} (recordings √ó features)\")\n",
    "print(f\"  y_train_motor:      {y_train_motor.shape} values\")\n",
    "print(f\"  y_train_total:      {y_train_total.shape} values\")\n",
    "print(f\"\\\\nTest set:\")\n",
    "print(f\"  X_test shape:       {X_test.shape}\")\n",
    "print(f\"  y_test_motor:       {y_test_motor.shape} values\")\n",
    "print(f\"  y_test_total:       {y_test_total.shape} values\")\n",
    "print(f\"\\\\nFeatures: {X_train.shape[1]} (all scaled with StandardScaler)\")\n",
    "print(f\"\\\\nüí° Note: Same X features used for both motor and total UPDRS prediction\")\n",
    "print(f\"   Only the target variable (y) changes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4833de",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ PART 1: PREDICTING motor_UPDRS (Movement Symptoms)\n",
    "\n",
    "### What we're predicting:\n",
    "- **Target:** motor_UPDRS score (0-108 scale)\n",
    "- **Meaning:** How severe are the patient's movement symptoms?\n",
    "  - Low score (~10-15): Mild tremor, slight stiffness\n",
    "  - Medium score (~20-30): Noticeable movement problems\n",
    "  - High score (~35-40): Severe motor impairment\n",
    "- **Input:** 26 voice features (jitter, shimmer, noise, complexity, age, test_time, etc.)\n",
    "- **Goal:** Build models that can predict this score from voice alone\n",
    "\n",
    "### Why this matters clinically:\n",
    "If we can predict motor_UPDRS accurately (R¬≤ > 0.80), patients can:\n",
    "- Record voice at home daily\n",
    "- Get instant motor symptom assessment\n",
    "- Detect deterioration early\n",
    "- Avoid weekly clinic visits ($200-500 per visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b041f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 1: LINEAR REGRESSION (Baseline Model)\n",
    "# ============================================================\n",
    "# What we're doing: Training a simple linear regression model\n",
    "# \n",
    "# How it works:\n",
    "#   motor_UPDRS = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Jitter) + Œ≤‚ÇÇ(Shimmer) + ... + Œ≤‚ÇÇ‚ÇÜ(feature‚ÇÇ‚ÇÜ)\n",
    "#\n",
    "# Assumptions:\n",
    "#   - Linear relationship between voice features and UPDRS\n",
    "#   - Each feature has constant effect (no interactions)\n",
    "#   - Example: \"1 unit increase in Jitter ‚Üí always adds 2.5 points to UPDRS\"\n",
    "#\n",
    "# Expected performance:\n",
    "#   - Decent baseline (R¬≤ ~0.70-0.75)\n",
    "#   - Won't capture non-linear patterns\n",
    "#   - Fast to train, easy to interpret\n",
    "#\n",
    "# Why it won't win:\n",
    "#   - Real relationship is NON-LINEAR\n",
    "#   - Misses feature interactions (e.g., high Jitter + high Shimmer is worse than sum)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Train on training data\n",
    "print(\"\\\\nTraining Linear Regression...\")\n",
    "lr_model.fit(X_train, y_train_motor)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "y_test_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(f\"\\\\nModel learned {X_train.shape[1]} coefficients (one per feature)\")\n",
    "print(f\"Intercept: {lr_model.intercept_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2991d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION BLOCK: LINEAR REGRESSION PERFORMANCE\n",
    "# ============================================================\n",
    "# What we're evaluating: How well does Linear Regression predict motor_UPDRS?\n",
    "#\n",
    "# Metrics we're calculating:\n",
    "# 1. R¬≤ (R-squared): % of variance explained\n",
    "# 2. RMSE (Root Mean Squared Error): Average error in UPDRS points\n",
    "# 3. MAE (Mean Absolute Error): Typical error magnitude\n",
    "# 4. MAPE (Mean Absolute Percentage Error): Error as percentage\n",
    "#\n",
    "# We calculate these for BOTH train and test:\n",
    "# - Train metrics: How well model fits training data\n",
    "# - Test metrics: How well model generalizes to new data\n",
    "# - Gap between train/test: Indicates overfitting\n",
    "#   * Small gap (< 5%): Good generalization\n",
    "#   * Large gap (> 15%): Model overfitting to training data\n",
    "\n",
    "print(\"\\\\n\" + \"-\"*60)\n",
    "print(\"üìä EVALUATION: LINEAR REGRESSION on motor_UPDRS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Calculate metrics for TRAINING set\n",
    "train_r2_lr = r2_score(y_train_motor, y_train_pred_lr)\n",
    "train_rmse_lr = np.sqrt(mean_squared_error(y_train_motor, y_train_pred_lr))\n",
    "train_mae_lr = mean_absolute_error(y_train_motor, y_train_pred_lr)\n",
    "train_mape_lr = np.mean(np.abs((y_train_motor - y_train_pred_lr) / y_train_motor)) * 100\n",
    "\n",
    "# Calculate metrics for TEST set\n",
    "test_r2_lr = r2_score(y_test_motor, y_test_pred_lr)\n",
    "test_rmse_lr = np.sqrt(mean_squared_error(y_test_motor, y_test_pred_lr))\n",
    "test_mae_lr = mean_absolute_error(y_test_motor, y_test_pred_lr)\n",
    "test_mape_lr = np.mean(np.abs((y_test_motor - y_test_pred_lr) / y_test_motor)) * 100\n",
    "\n",
    "print(\"\\\\nüìà TRAINING SET PERFORMANCE:\")\n",
    "print(f\"  R¬≤ Score:  {train_r2_lr:.4f}  ‚Üí Explains {train_r2_lr*100:.2f}% of motor_UPDRS variance\")\n",
    "print(f\"  RMSE:      {train_rmse_lr:.3f} points ‚Üí Average error magnitude\")\n",
    "print(f\"  MAE:       {train_mae_lr:.3f} points ‚Üí Typical error (less sensitive to outliers)\")\n",
    "print(f\"  MAPE:      {train_mape_lr:.2f}% ‚Üí Average percentage error\")\n",
    "\n",
    "print(\"\\\\nüìâ TEST SET PERFORMANCE (What really matters!):\")\n",
    "print(f\"  R¬≤ Score:  {test_r2_lr:.4f}  ‚Üí Explains {test_r2_lr*100:.2f}% of motor_UPDRS variance\")\n",
    "print(f\"  RMSE:      {test_rmse_lr:.3f} points ‚Üí Predicting within ~{test_rmse_lr:.1f} UPDRS points\")\n",
    "print(f\"  MAE:       {test_mae_lr:.3f} points ‚Üí Typical prediction error\")\n",
    "print(f\"  MAPE:      {test_mape_lr:.2f}% ‚Üí Average percentage error\")\n",
    "\n",
    "# Calculate overfitting gap\n",
    "r2_gap = (train_r2_lr - test_r2_lr) * 100\n",
    "rmse_gap = ((test_rmse_lr - train_rmse_lr) / train_rmse_lr) * 100\n",
    "\n",
    "print(\"\\\\n‚öñÔ∏è  OVERFITTING CHECK (Train vs Test):\")\n",
    "print(f\"  R¬≤ gap:    {r2_gap:.2f}% {'‚úÖ Good!' if r2_gap < 5 else '‚ö†Ô∏è Some overfitting' if r2_gap < 15 else '‚ùå Overfitting!'}\")\n",
    "print(f\"  RMSE increase: {rmse_gap:.2f}% on test set\")\n",
    "\n",
    "print(\"\\\\nüí° INTERPRETATION:\")\n",
    "if test_r2_lr > 0.80:\n",
    "    print(f\"  ‚úÖ Excellent! R¬≤ > 0.80 means model is clinically useful\")\n",
    "elif test_r2_lr > 0.70:\n",
    "    print(f\"  üëç Good baseline! R¬≤ = {test_r2_lr:.2f} is decent but can improve\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Weak performance. R¬≤ = {test_r2_lr:.2f} means model misses too much variance\")\n",
    "\n",
    "print(f\"\\\\n  Clinical meaning: Predictions are typically off by {test_mae_lr:.1f} UPDRS points\")\n",
    "print(f\"  For a patient with motor_UPDRS = 25, we'd predict {25-test_mae_lr:.1f} to {25+test_mae_lr:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: LINEAR REGRESSION - PREDICTED VS ACTUAL\n",
    "# ============================================================\n",
    "# What we're visualizing: How close are predictions to actual values?\n",
    "#\n",
    "# Perfect model: All points on diagonal line (predicted = actual)\n",
    "# Good model: Points clustered around diagonal\n",
    "# Bad model: Points scattered far from diagonal\n",
    "#\n",
    "# What to look for:\n",
    "# - Points above line: Model UNDERpredicts (says UPDRS is lower than reality)\n",
    "# - Points below line: Model OVERpredicts (says UPDRS is higher than reality)\n",
    "# - Spread: How much error/uncertainty\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set predictions\n",
    "axes[0].scatter(y_train_motor, y_train_pred_lr, alpha=0.5, s=20, color='blue', edgecolor='black', linewidth=0.5)\n",
    "axes[0].plot([y_train_motor.min(), y_train_motor.max()], [y_train_motor.min(), y_train_motor.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual motor_UPDRS', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted motor_UPDRS', fontsize=11)\n",
    "axes[0].set_title(f'Linear Regression: Training Set\\\\nR¬≤ = {train_r2_lr:.3f}, RMSE = {train_rmse_lr:.2f}', \n",
    "                  fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Test set predictions\n",
    "axes[1].scatter(y_test_motor, y_test_pred_lr, alpha=0.5, s=20, color='green', edgecolor='black', linewidth=0.5)\n",
    "axes[1].plot([y_test_motor.min(), y_test_motor.max()], [y_test_motor.min(), y_test_motor.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual motor_UPDRS', fontsize=11)\n",
    "axes[1].set_ylabel('Predicted motor_UPDRS', fontsize=11)\n",
    "axes[1].set_title(f'Linear Regression: Test Set\\\\nR¬≤ = {test_r2_lr:.3f}, RMSE = {test_rmse_lr:.2f}', \n",
    "                  fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° How to read this plot:\")\n",
    "print(\"  - Red dashed line = perfect predictions\")\n",
    "print(\"  - Points on the line = model predicted exactly right\")\n",
    "print(\"  - Points above line = underprediction (model says lower UPDRS than actual)\")\n",
    "print(\"  - Points below line = overprediction (model says higher UPDRS than actual)\")\n",
    "print(\"  - Closer to line = better model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: RESIDUAL ANALYSIS (Prediction Errors)\n",
    "# ============================================================\n",
    "# What are residuals? residual = actual - predicted (the error we made)\n",
    "#\n",
    "# Why analyze residuals?\n",
    "# - Check if errors are random or have patterns\n",
    "# - Good model: Residuals centered at 0, no trends\n",
    "# - Bad model: Residuals show patterns (model is biased)\n",
    "#\n",
    "# What we're looking for:\n",
    "# 1. Centered at zero: Mean residual should be ~0 (no systematic bias)\n",
    "# 2. Constant spread: Variance shouldn't change with predicted value (homoscedasticity)\n",
    "# 3. Normal distribution: Residuals should be bell-shaped\n",
    "# 4. No patterns: Random scatter (no trends, no curves)\n",
    "\n",
    "# Calculate residuals\n",
    "train_residuals_lr = y_train_motor - y_train_pred_lr\n",
    "test_residuals_lr = y_test_motor - y_test_pred_lr\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Residuals vs Predicted (Training)\n",
    "axes[0, 0].scatter(y_train_pred_lr, train_residuals_lr, alpha=0.5, s=20, color='blue')\n",
    "axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0, 0].set_xlabel('Predicted motor_UPDRS')\n",
    "axes[0, 0].set_ylabel('Residual (Actual - Predicted)')\n",
    "axes[0, 0].set_title('Residual Plot: Training Set\\\\n(Should be random scatter around 0)', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Predicted (Test)\n",
    "axes[0, 1].scatter(y_test_pred_lr, test_residuals_lr, alpha=0.5, s=20, color='green')\n",
    "axes[0, 1].axhline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0, 1].set_xlabel('Predicted motor_UPDRS')\n",
    "axes[0, 1].set_ylabel('Residual (Actual - Predicted)')\n",
    "axes[0, 1].set_title('Residual Plot: Test Set\\\\n(Check for patterns or trends)', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Residual distribution histogram (Training)\n",
    "axes[1, 0].hist(train_residuals_lr, bins=30, color='blue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(train_residuals_lr.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean = {train_residuals_lr.mean():.3f}')\n",
    "axes[1, 0].set_xlabel('Residual Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Residual Distribution: Training\\\\n(Should be normal, mean ‚âà 0)', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Residual distribution histogram (Test)\n",
    "axes[1, 1].hist(test_residuals_lr, bins=30, color='green', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(test_residuals_lr.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean = {test_residuals_lr.mean():.3f}')\n",
    "axes[1, 1].set_xlabel('Residual Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title(f'Residual Distribution: Test\\\\n(Should be normal, mean ‚âà 0)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nüìä RESIDUAL ANALYSIS SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set residuals: Mean = {train_residuals_lr.mean():.4f}, Std = {train_residuals_lr.std():.3f}\")\n",
    "print(f\"Test set residuals:     Mean = {test_residuals_lr.mean():.4f}, Std = {test_residuals_lr.std():.3f}\")\n",
    "print(\"\\\\nüí° What to look for:\")\n",
    "print(\"  ‚úÖ Mean ‚âà 0: No systematic bias (not consistently over/under predicting)\")\n",
    "print(\"  ‚úÖ Random scatter: No patterns in residual plots (model captured all relationships)\")\n",
    "print(\"  ‚úÖ Normal distribution: Bell-shaped histogram (assumptions valid)\")\n",
    "print(\"  ‚úÖ Constant spread: Similar variance across all predicted values\")\n",
    "\n",
    "if abs(test_residuals_lr.mean()) < 1.0:\n",
    "    print(\"\\\\n‚úÖ Residuals are well-centered (mean near 0)\")\n",
    "else:\n",
    "    print(f\"\\\\n‚ö†Ô∏è  Slight bias detected (mean = {test_residuals_lr.mean():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09fd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 2: RANDOM FOREST REGRESSION (Expected Winner!)\n",
    "# ============================================================\n",
    "# What we're doing: Training an ensemble of 100 decision trees\n",
    "#\n",
    "# How Random Forest works:\n",
    "# 1. Create 100 different decision trees\n",
    "# 2. Each tree trained on random subset of data (bootstrap sampling)\n",
    "# 3. Each tree considers random subset of features at each split\n",
    "# 4. Final prediction = average of all 100 tree predictions\n",
    "#\n",
    "# Why it's better than Linear Regression:\n",
    "# - Captures NON-LINEAR relationships (e.g., \"Jitter matters MORE when Shimmer is high\")\n",
    "# - Handles INTERACTIONS automatically (e.g., Jitter √ó Shimmer)\n",
    "# - Robust to OUTLIERS (averaging reduces their impact)\n",
    "# - Prevents OVERFITTING (bagging decorrelates trees)\n",
    "#\n",
    "# Expected performance:\n",
    "# - R¬≤ ~0.85-0.90 (much better than Linear Regression's ~0.73)\n",
    "# - RMSE ~2.5-3.5 points (vs Linear's ~4.0)\n",
    "# - Small train/test gap (good generalization)\n",
    "#\n",
    "# Bonus: Feature importance\n",
    "# - Tells us which voice features are most important\n",
    "# - Expected top features: HNR, Jitter(%), Shimmer\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ MODEL 2: RANDOM FOREST REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,        # 100 trees in the forest\n",
    "    max_depth=15,           # Maximum tree depth (prevent overfitting)\n",
    "    min_samples_split=20,   # Need 20 samples to split a node\n",
    "    min_samples_leaf=10,    # Minimum 10 samples in each leaf\n",
    "    max_features='sqrt',    # Consider ‚àö26 ‚âà 5 random features per split\n",
    "    random_state=42,        # Reproducibility\n",
    "    n_jobs=-1               # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\\\nTraining Random Forest (100 trees)...\")\n",
    "print(\"This may take a few seconds...\")\n",
    "rf_model.fit(X_train, y_train_motor)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(f\"\\\\nModel configuration:\")\n",
    "print(f\"  - Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"  - Max tree depth: {rf_model.max_depth}\")\n",
    "print(f\"  - Features per split: {rf_model.max_features}\")\n",
    "print(f\"  - Total features used: {X_train.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
