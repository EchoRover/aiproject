{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50edc831",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf827ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# PyTorch for Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Check PyTorch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47809f16",
   "metadata": {},
   "source": [
    "## 2. Load Merged Solar Power Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged datasets\n",
    "data_dir = Path('../datasets/solar/processed')\n",
    "\n",
    "plant1 = pd.read_csv(data_dir / 'plant1_merged.csv', parse_dates=['DATE_TIME'])\n",
    "plant2 = pd.read_csv(data_dir / 'plant2_merged.csv', parse_dates=['DATE_TIME'])\n",
    "\n",
    "print(\"Plant 1 Data:\")\n",
    "print(f\"  Shape: {plant1.shape}\")\n",
    "print(f\"  Columns: {list(plant1.columns)}\")\n",
    "print(f\"\\nPlant 2 Data:\")\n",
    "print(f\"  Shape: {plant2.shape}\")\n",
    "print(f\"  Columns: {list(plant2.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PLANT 1 - First 5 Rows:\")\n",
    "print(\"=\"*80)\n",
    "display(plant1.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Statistics:\")\n",
    "print(\"=\"*80)\n",
    "display(plant1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9f2fd",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa953d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_solar_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess solar power data:\n",
    "    - Extract time-based features\n",
    "    - Handle missing values\n",
    "    - Create additional features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df['HOUR'] = df['DATE_TIME'].dt.hour\n",
    "    df['DAY'] = df['DATE_TIME'].dt.day\n",
    "    df['MONTH'] = df['DATE_TIME'].dt.month\n",
    "    df['DAY_OF_WEEK'] = df['DATE_TIME'].dt.dayofweek\n",
    "    df['DAY_OF_YEAR'] = df['DATE_TIME'].dt.dayofyear\n",
    "    \n",
    "    # Cyclical encoding for hour and month\n",
    "    df['HOUR_SIN'] = np.sin(2 * np.pi * df['HOUR'] / 24)\n",
    "    df['HOUR_COS'] = np.cos(2 * np.pi * df['HOUR'] / 24)\n",
    "    df['MONTH_SIN'] = np.sin(2 * np.pi * df['MONTH'] / 12)\n",
    "    df['MONTH_COS'] = np.cos(2 * np.pi * df['MONTH'] / 12)\n",
    "    \n",
    "    # Create interaction features\n",
    "    if 'AMBIENT_TEMPERATURE' in df.columns and 'IRRADIATION' in df.columns:\n",
    "        df['TEMP_IRRAD_INTERACTION'] = df['AMBIENT_TEMPERATURE'] * df['IRRADIATION']\n",
    "    \n",
    "    if 'MODULE_TEMPERATURE' in df.columns and 'IRRADIATION' in df.columns:\n",
    "        df['MODULE_TEMP_IRRAD'] = df['MODULE_TEMPERATURE'] * df['IRRADIATION']\n",
    "    \n",
    "    # Is daytime flag (hour between 6 AM and 6 PM)\n",
    "    df['IS_DAYTIME'] = ((df['HOUR'] >= 6) & (df['HOUR'] <= 18)).astype(int)\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess Plant 1\n",
    "plant1_processed = preprocess_solar_data(plant1)\n",
    "\n",
    "print(\"âœ… Data preprocessing completed!\")\n",
    "print(f\"\\nPlant 1 - New shape: {plant1_processed.shape}\")\n",
    "print(f\"\\nNew features added: {[col for col in plant1_processed.columns if col not in plant1.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c3bd6",
   "metadata": {},
   "source": [
    "## 4. Prepare Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e625521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(df, target_col='DC_POWER'):\n",
    "    \"\"\"\n",
    "    Prepare data for machine learning models\n",
    "    \"\"\"\n",
    "    # Select features\n",
    "    exclude_cols = ['DATE_TIME', 'SOURCE_KEY', 'PLANT_ID', target_col]\n",
    "    if 'AC_POWER' in df.columns and target_col != 'AC_POWER':\n",
    "        exclude_cols.append('AC_POWER')\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Handle any remaining NaN values\n",
    "    X = X.fillna(X.median())\n",
    "    y = y.fillna(y.median())\n",
    "    \n",
    "    # Split data (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False  # shuffle=False to preserve time order\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_cols\n",
    "\n",
    "# Prepare data for Plant 1\n",
    "X_train, X_test, y_train, y_test, scaler, features = prepare_ml_data(plant1_processed)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"\\nFeatures used:\\n{features}\")\n",
    "print(f\"\\nTarget variable: DC_POWER\")\n",
    "print(f\"  Training range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "print(f\"  Testing range: [{y_test.min():.2f}, {y_test.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908571f",
   "metadata": {},
   "source": [
    "## 5. Model 1: Linear Regression\n",
    "\n",
    "**Theory:** Finds the best-fitting straight line through the data\n",
    "\n",
    "**Equation:** $\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "\n",
    "**Method:** Ordinary Least Squares (OLS) - minimizes sum of squared residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8841cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Linear Regression Model...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "y_test_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "lr_train_mse = mean_squared_error(y_train, y_train_pred_lr)\n",
    "lr_test_mse = mean_squared_error(y_test, y_test_pred_lr)\n",
    "lr_train_rmse = np.sqrt(lr_train_mse)\n",
    "lr_test_rmse = np.sqrt(lr_test_mse)\n",
    "lr_train_mae = mean_absolute_error(y_train, y_train_pred_lr)\n",
    "lr_test_mae = mean_absolute_error(y_test, y_test_pred_lr)\n",
    "lr_train_r2 = r2_score(y_train, y_train_pred_lr)\n",
    "lr_test_r2 = r2_score(y_test, y_test_pred_lr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LINEAR REGRESSION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Training Set:\")\n",
    "print(f\"   MSE:  {lr_train_mse:.4f}\")\n",
    "print(f\"   RMSE: {lr_train_rmse:.4f}\")\n",
    "print(f\"   MAE:  {lr_train_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {lr_train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Testing Set:\")\n",
    "print(f\"   MSE:  {lr_test_mse:.4f}\")\n",
    "print(f\"   RMSE: {lr_test_rmse:.4f}\")\n",
    "print(f\"   MAE:  {lr_test_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {lr_test_r2:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb0d0d",
   "metadata": {},
   "source": [
    "## 6. Model 2: Polynomial Regression (Degree 2)\n",
    "\n",
    "**Theory:** Extends linear regression by adding polynomial terms to capture non-linear relationships\n",
    "\n",
    "**Equation:** $\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\beta_3x_2 + \\beta_4x_2^2 + ...$\n",
    "\n",
    "**Method:** Create polynomial features, then apply linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5621cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Polynomial Regression Model (Degree 2)...\")\n",
    "\n",
    "# Create polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Polynomial features created: {X_train_poly.shape[1]} features (from {X_train.shape[1]} original)\")\n",
    "\n",
    "# Train polynomial regression\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_poly = poly_model.predict(X_train_poly)\n",
    "y_test_pred_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate\n",
    "poly_train_mse = mean_squared_error(y_train, y_train_pred_poly)\n",
    "poly_test_mse = mean_squared_error(y_test, y_test_pred_poly)\n",
    "poly_train_rmse = np.sqrt(poly_train_mse)\n",
    "poly_test_rmse = np.sqrt(poly_test_mse)\n",
    "poly_train_mae = mean_absolute_error(y_train, y_train_pred_poly)\n",
    "poly_test_mae = mean_absolute_error(y_test, y_test_pred_poly)\n",
    "poly_train_r2 = r2_score(y_train, y_train_pred_poly)\n",
    "poly_test_r2 = r2_score(y_test, y_test_pred_poly)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POLYNOMIAL REGRESSION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Training Set:\")\n",
    "print(f\"   MSE:  {poly_train_mse:.4f}\")\n",
    "print(f\"   RMSE: {poly_train_rmse:.4f}\")\n",
    "print(f\"   MAE:  {poly_train_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {poly_train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Testing Set:\")\n",
    "print(f\"   MSE:  {poly_test_mse:.4f}\")\n",
    "print(f\"   RMSE: {poly_test_rmse:.4f}\")\n",
    "print(f\"   MAE:  {poly_test_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {poly_test_r2:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2f02d",
   "metadata": {},
   "source": [
    "## 7. Model 3: Decision Tree Regressor\n",
    "\n",
    "**Theory:** Learns decision rules from data features to make predictions\n",
    "\n",
    "**Method:** Recursively splits data based on features to minimize variance\n",
    "\n",
    "**Advantages:** Can capture non-linear relationships without feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ce556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Decision Tree Regressor Model...\")\n",
    "\n",
    "dt_model = DecisionTreeRegressor(\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_dt = dt_model.predict(X_train)\n",
    "y_test_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "dt_train_mse = mean_squared_error(y_train, y_train_pred_dt)\n",
    "dt_test_mse = mean_squared_error(y_test, y_test_pred_dt)\n",
    "dt_train_rmse = np.sqrt(dt_train_mse)\n",
    "dt_test_rmse = np.sqrt(dt_test_mse)\n",
    "dt_train_mae = mean_absolute_error(y_train, y_train_pred_dt)\n",
    "dt_test_mae = mean_absolute_error(y_test, y_test_pred_dt)\n",
    "dt_train_r2 = r2_score(y_train, y_train_pred_dt)\n",
    "dt_test_r2 = r2_score(y_test, y_test_pred_dt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DECISION TREE REGRESSOR RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Training Set:\")\n",
    "print(f\"   MSE:  {dt_train_mse:.4f}\")\n",
    "print(f\"   RMSE: {dt_train_rmse:.4f}\")\n",
    "print(f\"   MAE:  {dt_train_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {dt_train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Testing Set:\")\n",
    "print(f\"   MSE:  {dt_test_mse:.4f}\")\n",
    "print(f\"   RMSE: {dt_test_rmse:.4f}\")\n",
    "print(f\"   MAE:  {dt_test_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {dt_test_r2:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf2208",
   "metadata": {},
   "source": [
    "## 8. Model 4: Random Forest Regressor\n",
    "\n",
    "**Theory:** Ensemble of decision trees - combines multiple trees to improve predictions\n",
    "\n",
    "**Method:** \n",
    "- Bootstrap aggregating (bagging) of decision trees\n",
    "- Random feature selection at each split\n",
    "- Final prediction = average of all tree predictions\n",
    "\n",
    "**Advantages:** More robust and accurate than single decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest Regressor Model...\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_train_mse = mean_squared_error(y_train, y_train_pred_rf)\n",
    "rf_test_mse = mean_squared_error(y_test, y_test_pred_rf)\n",
    "rf_train_rmse = np.sqrt(rf_train_mse)\n",
    "rf_test_rmse = np.sqrt(rf_test_mse)\n",
    "rf_train_mae = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "rf_test_mae = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "rf_train_r2 = r2_score(y_train, y_train_pred_rf)\n",
    "rf_test_r2 = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOM FOREST REGRESSOR RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Training Set:\")\n",
    "print(f\"   MSE:  {rf_train_mse:.4f}\")\n",
    "print(f\"   RMSE: {rf_train_rmse:.4f}\")\n",
    "print(f\"   MAE:  {rf_train_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {rf_train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Testing Set:\")\n",
    "print(f\"   MSE:  {rf_test_mse:.4f}\")\n",
    "print(f\"   RMSE: {rf_test_rmse:.4f}\")\n",
    "print(f\"   MAE:  {rf_test_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {rf_test_r2:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc233992",
   "metadata": {},
   "source": [
    "## 9. Model 5: Neural Network (PyTorch)\n",
    "\n",
    "**Theory:** Multi-layer perceptron with backpropagation\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer: Number of features\n",
    "- Hidden layer 1: 64 neurons + ReLU activation\n",
    "- Hidden layer 2: 32 neurons + ReLU activation  \n",
    "- Hidden layer 3: 16 neurons + ReLU activation\n",
    "- Output layer: 1 neuron (regression output)\n",
    "\n",
    "**Training:** Adam optimizer, MSE loss, backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e2327",
   "metadata": {},
   "source": [
    "### 9.1 Prepare Data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train.values).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test.values).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"âœ… Data converted to PyTorch tensors\")\n",
    "print(f\"\\nðŸ“¦ Batch size: {batch_size}\")\n",
    "print(f\"ðŸ“¦ Number of batches: {len(train_loader)}\")\n",
    "print(f\"\\nðŸ”¢ Tensor shapes:\")\n",
    "print(f\"   X_train: {X_train_tensor.shape}\")\n",
    "print(f\"   y_train: {y_train_tensor.shape}\")\n",
    "print(f\"   X_test: {X_test_tensor.shape}\")\n",
    "print(f\"   y_test: {y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397d8ca",
   "metadata": {},
   "source": [
    "### 9.2 Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fa284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarPowerNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SolarPowerNN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass with ReLU activations\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)  # No activation for regression output\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "nn_model = SolarPowerNN(input_size).to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEURAL NETWORK ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(nn_model)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in nn_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in nn_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5c0bc",
   "metadata": {},
   "source": [
    "### 9.3 Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING NEURAL NETWORK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: 0.001\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Loss function: MSE\")\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    nn_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = nn_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation loss\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = nn_model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor).item()\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7adcd",
   "metadata": {},
   "source": [
    "### 9.4 Evaluate Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred_nn_tensor = nn_model(X_train_tensor)\n",
    "    y_test_pred_nn_tensor = nn_model(X_test_tensor)\n",
    "\n",
    "# Convert to numpy\n",
    "y_train_pred_nn = y_train_pred_nn_tensor.cpu().numpy().flatten()\n",
    "y_test_pred_nn = y_test_pred_nn_tensor.cpu().numpy().flatten()\n",
    "\n",
    "# Evaluate\n",
    "nn_train_mse = mean_squared_error(y_train, y_train_pred_nn)\n",
    "nn_test_mse = mean_squared_error(y_test, y_test_pred_nn)\n",
    "nn_train_rmse = np.sqrt(nn_train_mse)\n",
    "nn_test_rmse = np.sqrt(nn_test_mse)\n",
    "nn_train_mae = mean_absolute_error(y_train, y_train_pred_nn)\n",
    "nn_test_mae = mean_absolute_error(y_test, y_test_pred_nn)\n",
    "nn_train_r2 = r2_score(y_train, y_train_pred_nn)\n",
    "nn_test_r2 = r2_score(y_test, y_test_pred_nn)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEURAL NETWORK RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Training Set:\")\n",
    "print(f\"   MSE:  {nn_train_mse:.4f}\")\n",
    "print(f\"   RMSE: {nn_train_rmse:.4f}\")\n",
    "print(f\"   MAE:  {nn_train_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {nn_train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Testing Set:\")\n",
    "print(f\"   MSE:  {nn_test_mse:.4f}\")\n",
    "print(f\"   RMSE: {nn_test_rmse:.4f}\")\n",
    "print(f\"   MAE:  {nn_test_mae:.4f}\")\n",
    "print(f\"   RÂ²:   {nn_test_r2:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457249a1",
   "metadata": {},
   "source": [
    "### 9.5 Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d874765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.title('Neural Network Training History', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('nn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Training history plot saved as 'nn_training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc8339",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9114c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression', 'Decision Tree', 'Random Forest', 'Neural Network'],\n",
    "    'Train_RMSE': [lr_train_rmse, poly_train_rmse, dt_train_rmse, rf_train_rmse, nn_train_rmse],\n",
    "    'Test_RMSE': [lr_test_rmse, poly_test_rmse, dt_test_rmse, rf_test_rmse, nn_test_rmse],\n",
    "    'Train_MAE': [lr_train_mae, poly_train_mae, dt_train_mae, rf_train_mae, nn_train_mae],\n",
    "    'Test_MAE': [lr_test_mae, poly_test_mae, dt_test_mae, rf_test_mae, nn_test_mae],\n",
    "    'Train_RÂ²': [lr_train_r2, poly_train_r2, dt_train_r2, rf_train_r2, nn_train_r2],\n",
    "    'Test_RÂ²': [lr_test_r2, poly_test_r2, dt_test_r2, rf_test_r2, nn_test_r2]\n",
    "})\n",
    "\n",
    "# Sort by Test RÂ² (descending)\n",
    "results = results.sort_values('Test_RÂ²', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON RESULTS - SOLAR POWER PREDICTION\")\n",
    "print(\"=\"*100)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Find best model\n",
    "best_model = results.iloc[0]['Model']\n",
    "best_r2 = results.iloc[0]['Test_RÂ²']\n",
    "best_rmse = results.iloc[0]['Test_RMSE']\n",
    "best_mae = results.iloc[0]['Test_MAE']\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model}\")\n",
    "print(f\"   Test RÂ² Score: {best_r2:.4f}\")\n",
    "print(f\"   Test RMSE: {best_rmse:.4f}\")\n",
    "print(f\"   Test MAE: {best_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562ebad",
   "metadata": {},
   "source": [
    "### 10.1 Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef880bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "x_pos = np.arange(len(results))\n",
    "axes[0].bar(x_pos - 0.2, results['Train_RMSE'], 0.4, label='Train', color='steelblue', alpha=0.8)\n",
    "axes[0].bar(x_pos + 0.2, results['Test_RMSE'], 0.4, label='Test', color='coral', alpha=0.8)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('RMSE (Lower is Better)', fontweight='bold')\n",
    "axes[0].set_title('Root Mean Squared Error', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(x_pos - 0.2, results['Train_MAE'], 0.4, label='Train', color='steelblue', alpha=0.8)\n",
    "axes[1].bar(x_pos + 0.2, results['Test_MAE'], 0.4, label='Test', color='coral', alpha=0.8)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('MAE (Lower is Better)', fontweight='bold')\n",
    "axes[1].set_title('Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# RÂ² comparison\n",
    "axes[2].bar(x_pos - 0.2, results['Train_RÂ²'], 0.4, label='Train', color='steelblue', alpha=0.8)\n",
    "axes[2].bar(x_pos + 0.2, results['Test_RÂ²'], 0.4, label='Test', color='green', alpha=0.8)\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[2].set_ylabel('RÂ² Score (Higher is Better)', fontweight='bold')\n",
    "axes[2].set_title('RÂ² Score', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Solar Power Prediction - Model Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Comparison plot saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da5d62",
   "metadata": {},
   "source": [
    "## 11. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c382aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for all models\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Sample indices for visualization (first 500 test points)\n",
    "sample_size = min(500, len(y_test))\n",
    "\n",
    "models_data = [\n",
    "    ('Linear Regression', y_test_pred_lr, lr_test_r2),\n",
    "    ('Polynomial Regression', y_test_pred_poly, poly_test_r2),\n",
    "    ('Decision Tree', y_test_pred_dt, dt_test_r2),\n",
    "    ('Random Forest', y_test_pred_rf, rf_test_r2),\n",
    "    ('Neural Network', y_test_pred_nn, nn_test_r2)\n",
    "]\n",
    "\n",
    "for idx, (name, pred, r2) in enumerate(models_data):\n",
    "    axes[idx].plot(y_test.values[:sample_size], label='Actual', linewidth=2, alpha=0.7)\n",
    "    axes[idx].plot(pred[:sample_size], label='Predicted', linewidth=2, alpha=0.7)\n",
    "    axes[idx].set_title(f'{name} (RÂ²={r2:.4f})', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Sample Index')\n",
    "    axes[idx].set_ylabel('DC Power')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide last subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Solar Power Prediction - All Models Comparison', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Predictions plot saved as 'predictions_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333c164",
   "metadata": {},
   "source": [
    "## 12. Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = min(15, len(feature_importance))\n",
    "plt.barh(feature_importance['Feature'][:top_n], feature_importance['Importance'][:top_n], color='steelblue')\n",
    "plt.xlabel('Importance', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Feature', fontweight='bold', fontsize=12)\n",
    "plt.title(f'Top {top_n} Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Feature importance plot saved as 'feature_importance.png'\")\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7355914",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e60555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SOLAR POWER PREDICTION - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Models Trained:\")\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"  {idx+1}. {row['Model']}:\")\n",
    "    print(f\"     Test RÂ² = {row['Test_RÂ²']:.4f}, RMSE = {row['Test_RMSE']:.4f}, MAE = {row['Test_MAE']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ† Best Performing Model: {best_model}\")\n",
    "print(f\"   Test RÂ² Score: {best_r2:.4f}\")\n",
    "print(f\"   Test RMSE: {best_rmse:.4f}\")\n",
    "print(f\"   Test MAE: {best_mae:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“ Generated Outputs:\")\n",
    "print(\"  - model_comparison.png\")\n",
    "print(\"  - predictions_comparison.png\")\n",
    "print(\"  - feature_importance.png\")\n",
    "print(\"  - nn_training_history.png\")\n",
    "\n",
    "print(\"\\nâœ… All models successfully trained and evaluated!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
