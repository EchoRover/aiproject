{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e7d7d9",
   "metadata": {},
   "source": [
    "# Random Forest Regressor - Household Power Consumption\n",
    "\n",
    "**Algorithm 4 of 7**\n",
    "\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to improve prediction accuracy and reduce overfitting.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Bootstrap Aggregating (Bagging)\n",
    "- Feature randomness\n",
    "- Voting/averaging across trees\n",
    "- Out-of-bag error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94048ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c6936",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/processed/household_preprocessed.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data['X_train_scaled']\n",
    "X_test = data['X_test_scaled']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b790b33",
   "metadata": {},
   "source": [
    "## 2. Random Forest Theory\n",
    "\n",
    "**How Random Forest Works:**\n",
    "\n",
    "1. **Bootstrap Sampling:** Create multiple random subsets of training data (sampling with replacement)\n",
    "\n",
    "2. **Build Decision Trees:** Train a decision tree on each bootstrap sample\n",
    "   - At each split, consider only a random subset of features\n",
    "   - This introduces randomness and decorrelates the trees\n",
    "\n",
    "3. **Aggregate Predictions:** Average predictions from all trees\n",
    "   - For regression: Mean of all tree predictions\n",
    "   - For classification: Majority vote\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$\\hat{y} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_b(x)$$\n",
    "\n",
    "Where:\n",
    "- $B$ = number of trees\n",
    "- $\\hat{f}_b(x)$ = prediction from tree $b$\n",
    "- $\\hat{y}$ = final ensemble prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b389ae",
   "metadata": {},
   "source": [
    "## 3. Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae258791",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model with 100 trees\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    max_depth=15,          # Maximum depth of each tree\n",
    "    min_samples_split=5,   # Minimum samples to split a node\n",
    "    random_state=42,       # For reproducibility\n",
    "    n_jobs=-1              # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining Random Forest with 100 trees...\")\n",
    "rf_model.fit(X_train, y_train.values.ravel())\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nModel trained with {rf_model.n_estimators} trees\")\n",
    "print(f\"Maximum tree depth: {rf_model.max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba50991",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a22198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for training set\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Training Set:\")\n",
    "print(f\"   R¬≤ Score: {train_r2:.4f}\")\n",
    "print(f\"   RMSE: {train_rmse:.4f}\")\n",
    "print(f\"   MAE: {train_mae:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Test Set:\")\n",
    "print(f\"   R¬≤ Score: {test_r2:.4f}\")\n",
    "print(f\"   RMSE: {test_rmse:.4f}\")\n",
    "print(f\"   MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\nüîç Overfitting Check:\")\n",
    "r2_diff = train_r2 - test_r2\n",
    "if r2_diff < 0.05:\n",
    "    print(f\"   ‚úÖ Good generalization (R¬≤ difference: {r2_diff:.4f})\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Potential overfitting (R¬≤ difference: {r2_diff:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cd2f1",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': data['feature_names'],\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance['Importance'].values, \n",
    "         color='steelblue', edgecolor='black')\n",
    "plt.yticks(range(len(feature_importance)), feature_importance['Feature'].values)\n",
    "plt.xlabel('Importance Score', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Features', fontweight='bold', fontsize=12)\n",
    "plt.title('Random Forest - Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüîë Most Important Feature: {feature_importance.iloc[0]['Feature']} ({feature_importance.iloc[0]['Importance']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731862c8",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Power (kW)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Power (kW)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title(f'Random Forest: Predicted vs Actual (R¬≤ = {test_r2:.4f})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test.values.ravel() - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.5, s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Power (kW)', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title('Residuals Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e693106",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Random Forest Results:**\n",
    "- Algorithm 4 of 7 successfully implemented\n",
    "- Ensemble of 100 decision trees\n",
    "- Strong performance with R¬≤ = {test_r2:.4f}\n",
    "\n",
    "**Key Advantages:**\n",
    "- Reduces overfitting compared to single decision tree\n",
    "- Provides feature importance rankings\n",
    "- Robust to outliers and noise\n",
    "- Handles non-linear relationships well\n",
    "\n",
    "**Applications:**\n",
    "- Excellent for power consumption prediction\n",
    "- Feature importance helps identify key factors\n",
    "- Can guide energy efficiency improvements"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
