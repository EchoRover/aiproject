{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aef0334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcbc8c",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facc2ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "âœ… Loaded 2,075,259 rows\n",
      "âœ… Loaded 2,075,259 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../datasets/household_power_consumption.txt', \n",
    "                 sep=';',\n",
    "                 low_memory=False,\n",
    "                 na_values=['?'],\n",
    "                 parse_dates={'datetime': ['Date', 'Time']},\n",
    "                 infer_datetime_format=True)\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f1f8c",
   "metadata": {},
   "source": [
    "## 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c2cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing before: 181,853\n",
      "Missing after: 0\n",
      "Rows remaining: 2,049,280\n"
     ]
    }
   ],
   "source": [
    "# Convert to numeric\n",
    "numeric_cols = ['Global_active_power', 'Global_reactive_power', 'Voltage', \n",
    "                'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(f\"Missing before: {df.isnull().sum().sum():,}\")\n",
    "\n",
    "# Drop rows with missing values (only ~1.25%)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Missing after: {df.isnull().sum().sum()}\")\n",
    "print(f\"Rows remaining: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f1467",
   "metadata": {},
   "source": [
    "## 3. Create Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa593d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Time features created:\n",
      "             datetime  hour  dayofweek  is_weekend  month  season\n",
      "0 2006-12-16 17:24:00    17          5           1     12       1\n",
      "1 2006-12-16 17:25:00    17          5           1     12       1\n",
      "2 2006-12-16 17:26:00    17          5           1     12       1\n",
      "3 2006-12-16 17:27:00    17          5           1     12       1\n",
      "4 2006-12-16 17:28:00    17          5           1     12       1\n"
     ]
    }
   ],
   "source": [
    "# Extract time features\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['quarter'] = df['datetime'].dt.quarter\n",
    "df['year'] = df['datetime'].dt.year\n",
    "\n",
    "# Is weekend?\n",
    "df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "\n",
    "# Season (1=Winter, 2=Spring, 3=Summer, 4=Fall)\n",
    "df['season'] = df['month'].apply(lambda x: (\n",
    "    1 if x in [12, 1, 2] else\n",
    "    2 if x in [3, 4, 5] else\n",
    "    3 if x in [6, 7, 8] else 4\n",
    "))\n",
    "\n",
    "print(\"âœ… Time features created:\")\n",
    "print(df[['datetime', 'hour', 'dayofweek', 'is_weekend', 'month', 'season']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d57f7",
   "metadata": {},
   "source": [
    "## 4. Sample Data (For Faster Training)\n",
    "\n",
    "2M rows is too large - sample 100k rows for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1257db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sampled 100,000 rows\n",
      "Date range: 2006-12-16 17:48:00 to 2010-11-26 20:19:00\n"
     ]
    }
   ],
   "source": [
    "# Stratified sample to preserve temporal patterns\n",
    "SAMPLE_SIZE = 100000\n",
    "\n",
    "df_sampled = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "df_sampled = df_sampled.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Sampled {len(df_sampled):,} rows\")\n",
    "print(f\"Date range: {df_sampled['datetime'].min()} to {df_sampled['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f094982",
   "metadata": {},
   "source": [
    "## 5. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca99a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (100000, 10)\n",
      "Target shape: (100000,)\n",
      "\n",
      "Features used: ['Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', 'hour', 'dayofweek', 'is_weekend', 'month', 'season']\n"
     ]
    }
   ],
   "source": [
    "# Features for modeling\n",
    "feature_cols = ['Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3',\n",
    "                'hour', 'dayofweek', 'is_weekend', 'month', 'season']\n",
    "\n",
    "X = df_sampled[feature_cols].copy()\n",
    "y = df_sampled['Global_active_power'].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b74537",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824a9d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 80,000 samples\n",
      "Testing set: 20,000 samples\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af9c2c",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394ae054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features scaled (StandardScaler)\n",
      "\n",
      "Mean after scaling: 0.000000\n",
      "Std after scaling: 1.000000\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Features scaled (StandardScaler)\")\n",
    "print(f\"\\nMean after scaling: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std after scaling: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52031095",
   "metadata": {},
   "source": [
    "## 8. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fae061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessed data saved to: ../datasets/processed/household_preprocessed.pkl\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   â€¢ Train samples: 80,000\n",
      "   â€¢ Test samples: 20,000\n",
      "   â€¢ Features: 10\n",
      "   â€¢ Target: Global_active_power (kW)\n",
      "\n",
      "âœ… Ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "# Save to pickle for next notebooks\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': feature_cols,\n",
    "    'scaler': scaler\n",
    "}\n",
    "\n",
    "with open('../datasets/processed/household_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"âœ… Preprocessed data saved to: ../datasets/processed/household_preprocessed.pkl\")\n",
    "print(\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   â€¢ Train samples: {len(X_train):,}\")\n",
    "print(f\"   â€¢ Test samples: {len(X_test):,}\")\n",
    "print(f\"   â€¢ Features: {len(feature_cols)}\")\n",
    "print(f\"   â€¢ Target: Global_active_power (kW)\")\n",
    "print(\"\\nâœ… Ready for modeling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
