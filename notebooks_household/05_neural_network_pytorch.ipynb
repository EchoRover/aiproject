{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ddbff05",
   "metadata": {},
   "source": [
    "# Neural Network (PyTorch) - Power Consumption Prediction\n",
    "\n",
    "Deep learning approach using PyTorch to predict household power consumption.\n",
    "\n",
    "**Architecture:**\n",
    "- Multi-layer feedforward neural network\n",
    "- ReLU activation functions\n",
    "- Dropout for regularization\n",
    "- Adam optimizer\n",
    "\n",
    "**Goal:** Beat traditional ML models using deep learning âœ… REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Libraries loaded\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2df8ff",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb47754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/processed/household_preprocessed.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data['X_train_scaled']\n",
    "X_test = data['X_test_scaled']\n",
    "y_train = data['y_train'].values\n",
    "y_test = data['y_test'].values\n",
    "\n",
    "print(f\"Training: {X_train.shape}\")\n",
    "print(f\"Testing: {X_test.shape}\")\n",
    "print(f\"Features: {len(data['feature_names'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af91506",
   "metadata": {},
   "source": [
    "## 2. Convert to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04feee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1).to(device)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"âœ… Data converted to tensors\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054bdcfa",
   "metadata": {},
   "source": [
    "## 3. Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420580a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerPredictionNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PowerPredictionNN, self).__init__()\n",
    "        \n",
    "        # Layer 1: Input -> 128\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Layer 2: 128 -> 64\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Layer 3: 64 -> 32\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Output layer: 32 -> 1\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "model = PowerPredictionNN(input_size).to(device)\n",
    "\n",
    "print(\"âœ… Neural Network Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73cc8f",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "print(\"âœ… Training setup complete\")\n",
    "print(f\"Loss function: MSE\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fce6e6",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2048812",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING NEURAL NETWORK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f82eb",
   "metadata": {},
   "source": [
    "## 6. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a030a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontweight='bold', fontsize=12)\n",
    "plt.title('Training History', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c581e7b",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "nn_r2 = r2_score(y_test, y_pred)\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "nn_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RÂ² Score: {nn_r2:.4f}\")\n",
    "print(f\"RMSE: {nn_rmse:.4f}\")\n",
    "print(f\"MAE: {nn_mae:.4f}\")\n",
    "\n",
    "# Compare with baseline (from notebook 03)\n",
    "print(\"\\nðŸ“Š COMPARISON WITH TRADITIONAL ML:\")\n",
    "print(\"(Approximate values from previous notebooks)\")\n",
    "print(\"   Linear Regression RÂ²: ~0.90\")\n",
    "print(\"   Decision Tree RÂ²: ~0.85\")\n",
    "print(f\"   Neural Network RÂ²: {nn_r2:.4f}\")\n",
    "\n",
    "if nn_r2 > 0.90:\n",
    "    print(\"\\nðŸŽ¯ Neural Network OUTPERFORMS traditional ML!\")\n",
    "else:\n",
    "    print(\"\\nðŸ“Œ Neural Network comparable to traditional ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac28da7",
   "metadata": {},
   "source": [
    "## 8. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5aedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Power (kW)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Power (kW)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title(f'Predicted vs Actual (RÂ² = {nn_r2:.4f})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5, s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Power (kW)', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontweight='bold', fontsize=12)\n",
    "axes[1].set_title('Residuals Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68978430",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1db66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = '../models/neural_network_household.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'r2_score': nn_r2,\n",
    "    'rmse': nn_rmse,\n",
    "    'mae': nn_mae,\n",
    "    'input_size': input_size\n",
    "}, model_path)\n",
    "\n",
    "print(f\"âœ… Model saved to {model_path}\")\n",
    "print(f\"   RÂ² Score: {nn_r2:.4f}\")\n",
    "print(f\"   RMSE: {nn_rmse:.4f}\")\n",
    "print(f\"   MAE: {nn_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcb03a",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Neural Network Results:**\n",
    "- âœ… **PyTorch Neural Network** - REQUIRED model implemented\n",
    "- Deep learning architecture with 4 layers (128â†’64â†’32â†’1)\n",
    "- Batch normalization and dropout for regularization\n",
    "- Adam optimizer with learning rate scheduling\n",
    "\n",
    "**Performance:**\n",
    "- Achieved competitive results with traditional ML\n",
    "- Can capture non-linear relationships in power consumption\n",
    "- Useful for complex pattern recognition in energy data\n",
    "\n",
    "**Advantages:**\n",
    "- Flexible architecture for complex patterns\n",
    "- Can be extended for time-series forecasting\n",
    "- Scalable to larger datasets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
